---
title: "Data 612 Project 4"
author: "Anthony Pagan"
date: "7/1/2020"
output: 
    html_document:
        toc: true
        toc_float: true
        toc_depth: 5
        css: style.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
#knitr::opts_chunk$set(echo = TRUE)
require(knitr)
library(ggplot2)
library(tidyr)
library(MASS)
library(psych)
library(kableExtra)
library(dplyr)
library(faraway)
library(gridExtra)
library(reshape2)
library(leaps)
library(pROC)
library(caret)
library(naniar)
library(pander)
library(pROC)
library(mlbench)
library(e1071)
library(fpp2)
library(mlr)
library(recommenderlab)
library(jsonlite)
library(stringr)
```

The goal of this assignment is give you practice working with accuracy and other recommender system metrics.

Deliverables:
- As in your previous assignments, compare the accuracy of at least two recommender system algorithms against your offline data.
- Implement support for at least one business or user experience goal such as increased serendipity, novelty, or diversity.
- Compare and report on any change in accuracy before and after youâ€™ve made the change in #2.
- As part of your textual conclusion, discuss one or more additional experiments that could be performed and/or metrics that could be evaluated only if online evaluation was possible. Also, briefly propose how you would design a reasonable online evaluation environment.

# Get Data 

The dataset was retrieved from kaggle website. The data includes user ratings for different Restaurants from 0-2 with 2 being the highest rating. We are only interested in the name of the device , username and rating, so we extract only those columns as a start.

Restaurant Review Dataset

https://www.kaggle.com/uciml/restaurant-data-with-consumer-ratings

```{r}
rm(list=ls())#removes all variables previously stored

urate<- read.csv("https://raw.githubusercontent.com/apag101/Data612/master/Projects/Project4/rating_final.csv", header=TRUE)

glimpse(urate)
describe(urate)

table(as.vector(urate$rating))
```


# Create Matrix
```{r}
set.seed(123)
data.mat <- matrix(data=urate$rating,ncol=length(unique(urate$placeID)),nrow=length(unique(urate$userID)))
rownames(data.mat)<-c(paste(unique(urate$userIDD)))
colnames(data.mat)<-c(paste(unique(urate$placeID)))
glimpse(data.mat)
rdata.mat<-as(data.mat, "realRatingMatrix")
View(rdata.mat@data)
```


## Review
```{r}
table(as.vector(rdata.mat@data))
vector_rates<-as.vector(rdata.mat@data)
vector_rates<-factor(vector_rates)
qplot(vector_rates)+ggtitle("Distribution of ratings")
image(rdata.mat[1:80, 1:130])
similarity(rdata.mat[1:5,],method="cosine", which="userID")
```

# Partition Data
```{r}
percentage_training<-.8
min(rowCounts(rdata.mat))

items_to_keep<- 15
rating_thresold<- 2
n_eval<-1

eval_sets <- evaluationScheme(data=rdata.mat, method="split", train=percentage_training, given= items_to_keep, goodRating = rating_thresold, k=n_eval)
eval_sets
```


# Bootstrapping Test

```{r}
eval_sets2 <- evaluationScheme(data=rdata.mat, method="bootstrap", train=percentage_training, given= items_to_keep, goodRating = rating_thresold, k=n_eval)
eval_sets2
per_test<-nrow(getData(eval_sets2, "known")) /nrow(rdata.mat)
per_test

per_train<-length(unique(eval_sets2@runsTrain[[1]]))/nrow(rdata.mat)
per_test+per_train

table_train<-table(eval_sets2@runsTrain[[1]])
n_repetitions<-factor(as.vector(table_train))
qplot(n_repetitions)+ggtitle("Number of repetitiions in the training set")
```


# Cross Validation K-fold 
```{r}
n_fold<-4
eval_sets3<-evaluationScheme(data=rdata.mat, method="cross-validation", k=n_fold, given= items_to_keep, goodRating = rating_thresold)
size_sets<-sapply(eval_sets3@runsTrain, length)
size_sets
```


# Evaluation Techniques
```{r}

model_to_evaluate<- "IBCF"
model_parameters<-NULL

eval_recommender <- Recommender(data=getData(eval_sets3, "train"), method=model_to_evaluate, parameter=model_parameters)
items_to_recommend<-10

eval_prediction<-predict(object= eval_recommender, newdata=getData(eval_sets3, "known"), n=items_to_recommend, type="ratings")

qplot(rowCounts(eval_prediction)) + geom_histogram(binwidth = 10) + ggtitle("Distribution of movies per user")

eval_accuracy<- calcPredictionAccuracy(x = eval_prediction, data=getData(eval_sets3, "unknown"), byUser=TRUE)
head(eval_accuracy)

qplot(eval_accuracy[,"RMSE"]) + geom_histogram(binwidth = .1) + ggtitle("Distribution of the RMSE by user")

eval_accuracy2<- calcPredictionAccuracy(x = eval_prediction, data=getData(eval_sets3, "unknown"), byUser=FALSE)
eval_accuracy2
```


#Evaluate Recommendations

```{r}
results<-evaluate(x= eval_sets3, method= model_to_evaluate, n=seq(10,100,10))
head(getConfusionMatrix(results)[[1]])
columns_to_sum<-c("TP","FP","FN","TN")
indices_summed<-Reduce("+",getConfusionMatrix(results))[,columns_to_sum]
head(indices_summed)
plot(results, annotate= TRUE, main= "ROC Curve")
plot(results, "prec/rec", annotate=TRUE, main="Precision-recall")

models_to_evaluate<- list(
    IBCF_cos = list(name="IBCF", param=list(method="cosine")),
    IBCF_cor = list(name="IBCF", param=list(method="pearson")),
    UBCF_cos = list(name="UBCF", param=list(method="cosine")),
    IBCF_cor = list(name="IBCF", param=list(method="pearson")),
    IBCF_cos = list(name="RANDOM", param=NULL))
n_recommendations<-c(1,5,seq(10,100,10))

list_results<-evaluate(x=eval_sets3, method=models_to_evaluate, n=n_recommendations)

sapply(list_results, class)== "evaluationResults"
avg_matrices<-lapply(list_results,avg)
head(avg_matrices$IBCF_cos[,5:8])
```


## Select Model

```{r}
plot(list_results, annotate=1, legend="topleft") 
title("ROC curve")
plot(list_results,"prec/rec", annotate=1, legend="bottomright") 
title("Precision-recall")
```

## Select best parameter

```{r}
vector_k <- c(5,10,20,30,40)
models_to_evaluate <- lapply(vector_k, function(k){
    list(name="IBCF", param=list(method="cosine", k=k))
})
names(models_to_evaluate)<-paste0("IBCF_k_", vector_k)
list_results<-evaluate(x=eval_sets3, method=models_to_evaluate, n=n_recommendations)
plot(list_results, annotate=1, legend="topleft") 
title("ROC curve")
plot(list_results,"prec/rec", annotate=1, legend="bottomright") 
title("Precision-recall")
```

# Conclusion



## References

https://blog.exploratory.io/working-with-json-data-in-very-simple-way-ad7ebcc0bb89


## APPENDIX

**Code used in analysis**
```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}

```

